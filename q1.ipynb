{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPRkqHmazmS9klJXnzMh4ff",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NIRMALT04/DND/blob/main/q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jfAuoj06trS",
        "outputId": "8c901898-be7a-493f-eba9-24bd49e0637e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct  2 17:19:11 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "import math, time, random, os\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR-10: 50k train, 10k test; images are 32x32x3\n",
        "IMG_SIZE = 32\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Stronger aug helps ViT. Start simple; then try RandAugment/AutoAugment/CutMix/MixUp later.\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "test_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_tf)\n",
        "test_ds  = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuZ55e8d8MbB",
        "outputId": "bfbf531f-9dc6-4681-de92-b2d747926d0b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 41.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ViTConfig:\n",
        "    image_size: int = 32\n",
        "    patch_size: int = 4\n",
        "    in_chans: int = 3\n",
        "    num_classes: int = 10\n",
        "    dim: int = 256\n",
        "    depth: int = 6\n",
        "    heads: int = 8\n",
        "    mlp_ratio: int = 4\n",
        "    attn_dropout: float = 0.0\n",
        "    dropout: float = 0.1\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_chans, dim):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_chans, dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, C, H, W] -> [B, N, D]\n",
        "        x = self.proj(x)       # [B, D, H/P, W/P]\n",
        "        x = x.flatten(2)       # [B, D, N]\n",
        "        x = x.transpose(1, 2)  # [B, N, D]\n",
        "        return x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fn(self.norm(x))\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=8, dropout=0.0):\n",
        "        super().__init__()\n",
        "        assert dim % heads == 0\n",
        "        self.heads = heads\n",
        "        self.scale = (dim // heads) ** -0.5\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape\n",
        "        qkv = self.to_qkv(x).reshape(B, N, 3, self.heads, D // self.heads)\n",
        "        q, k, v = qkv.unbind(dim=2)  # [B, N, H, Dh]\n",
        "        q = q.transpose(1, 2)  # [B, H, N, Dh]\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.drop(attn)\n",
        "        out = attn @ v  # [B, H, N, Dh]\n",
        "        out = out.transpose(1, 2).reshape(B, N, D)\n",
        "        return self.proj(out)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, heads, mlp_ratio, attn_dropout=0.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = PreNorm(dim, MultiHeadSelfAttention(dim, heads, attn_dropout))\n",
        "        self.mlp  = PreNorm(dim, MLP(dim, dim * mlp_ratio, dropout))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(x)\n",
        "        x = x + self.mlp(x)\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, cfg: ViTConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.patch_embed = PatchEmbed(cfg.image_size, cfg.patch_size, cfg.in_chans, cfg.dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, cfg.dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + num_patches, cfg.dim))\n",
        "        self.pos_drop = nn.Dropout(cfg.dropout)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(cfg.dim, cfg.heads, cfg.mlp_ratio, cfg.attn_dropout, cfg.dropout)\n",
        "            for _ in range(cfg.depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(cfg.dim)\n",
        "        self.head = nn.Linear(cfg.dim, cfg.num_classes)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.LayerNorm):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)               # [B, N, D]\n",
        "        cls = self.cls_token.expand(B, -1, -1)  # [B, 1, D]\n",
        "        x = torch.cat([cls, x], dim=1)        # [B, 1+N, D]\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        cls_out = x[:, 0]                     # CLS token\n",
        "        return self.head(cls_out)\n"
      ],
      "metadata": {
        "id": "WvNSROdD8Wvz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(logits, y):\n",
        "    preds = logits.argmax(dim=1)\n",
        "    return (preds == y).float().mean().item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        total += x.size(0)\n",
        "    return correct / total, loss_sum / total\n",
        "\n",
        "def save_ckpt(model, optimizer, epoch, best_acc, path='vit_cifar10.pt'):\n",
        "    torch.save({'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'best_acc': best_acc}, path)\n",
        "\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
      ],
      "metadata": {
        "id": "b8Izfjrl8kSG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 60\n",
        "cfg = ViTConfig(\n",
        "    image_size=IMG_SIZE,\n",
        "    patch_size=4,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    dim=256,\n",
        "    depth=6,\n",
        "    heads=8,\n",
        "    mlp_ratio=4,\n",
        "    attn_dropout=0.0,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "model = ViT(cfg).to(device)\n",
        "print('Params (M):', count_params(model) / 1e6)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05, betas=(0.9, 0.999))\n",
        "# Cosine LR with warmup\n",
        "total_steps = EPOCHS * len(train_loader)\n",
        "warmup_steps = int(0.05 * total_steps)\n",
        "\n",
        "def lr_schedule(step):\n",
        "    if step < warmup_steps:\n",
        "        return step / max(1, warmup_steps)\n",
        "    pct = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
        "    return 0.5 * (1 + math.cos(math.pi * pct))\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "best_acc = 0.0\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    epoch_loss, epoch_acc, n = 0.0, 0.0, 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        lr = 3e-4 * lr_schedule(global_step)\n",
        "        for pg in optimizer.param_groups:\n",
        "            pg['lr'] = lr\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.cuda.amp.autocast(enabled=device.type=='cuda'):\n",
        "            logits = model(x)\n",
        "            loss = F.cross_entropy(logits, y, label_smoothing=0.1)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        acc = accuracy(logits, y)\n",
        "        bs = x.size(0)\n",
        "        epoch_loss += loss.item() * bs\n",
        "        epoch_acc += acc * bs\n",
        "        n += bs\n",
        "        global_step += 1\n",
        "\n",
        "    tr_loss = epoch_loss / n\n",
        "    tr_acc  = epoch_acc / n\n",
        "    te_acc, te_loss = evaluate(model, test_loader)\n",
        "\n",
        "    if te_acc > best_acc:\n",
        "        best_acc = te_acc\n",
        "        save_ckpt(model, optimizer, epoch, best_acc)\n",
        "\n",
        "    print(f'Epoch {epoch:03d} | LR {lr:.6f} | Train Loss {tr_loss:.4f} Acc {tr_acc*100:.2f}% | '\n",
        "          f'Test Loss {te_loss:.4f} Acc {te_acc*100:.2f}% | Best {best_acc*100:.2f}%')\n",
        "\n",
        "print('Best Test Acc:', best_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slReOiJf8rxH",
        "outputId": "42b3b83a-6254-41b0-8583-ed6ae414dd8f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params (M): 4.766474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-384599124.py:28: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/tmp/ipython-input-384599124.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=device.type=='cuda'):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | LR 0.000100 | Train Loss 2.0267 Acc 26.92% | Test Loss 1.7669 Acc 34.82% | Best 34.82%\n",
            "Epoch 002 | LR 0.000200 | Train Loss 1.8102 Acc 37.75% | Test Loss 1.6180 Acc 41.92% | Best 41.92%\n",
            "Epoch 003 | LR 0.000300 | Train Loss 1.6218 Acc 47.78% | Test Loss 1.3846 Acc 50.18% | Best 50.18%\n",
            "Epoch 004 | LR 0.000300 | Train Loss 1.5071 Acc 53.16% | Test Loss 1.2462 Acc 55.98% | Best 55.98%\n",
            "Epoch 005 | LR 0.000299 | Train Loss 1.4365 Acc 56.64% | Test Loss 1.1785 Acc 59.28% | Best 59.28%\n",
            "Epoch 006 | LR 0.000298 | Train Loss 1.3866 Acc 59.33% | Test Loss 1.0848 Acc 63.32% | Best 63.32%\n",
            "Epoch 007 | LR 0.000296 | Train Loss 1.3428 Acc 61.40% | Test Loss 1.1353 Acc 60.77% | Best 63.32%\n",
            "Epoch 008 | LR 0.000294 | Train Loss 1.3012 Acc 63.19% | Test Loss 1.0248 Acc 64.33% | Best 64.33%\n",
            "Epoch 009 | LR 0.000292 | Train Loss 1.2712 Acc 64.64% | Test Loss 1.0173 Acc 64.89% | Best 64.89%\n",
            "Epoch 010 | LR 0.000289 | Train Loss 1.2330 Acc 66.60% | Test Loss 0.9670 Acc 67.10% | Best 67.10%\n",
            "Epoch 011 | LR 0.000286 | Train Loss 1.2073 Acc 67.71% | Test Loss 0.9069 Acc 68.75% | Best 68.75%\n",
            "Epoch 012 | LR 0.000282 | Train Loss 1.1750 Acc 69.28% | Test Loss 0.8933 Acc 69.42% | Best 69.42%\n",
            "Epoch 013 | LR 0.000278 | Train Loss 1.1515 Acc 70.30% | Test Loss 0.8596 Acc 71.43% | Best 71.43%\n",
            "Epoch 014 | LR 0.000273 | Train Loss 1.1303 Acc 71.52% | Test Loss 0.8511 Acc 71.68% | Best 71.68%\n",
            "Epoch 015 | LR 0.000268 | Train Loss 1.1071 Acc 72.52% | Test Loss 0.8342 Acc 71.93% | Best 71.93%\n",
            "Epoch 016 | LR 0.000263 | Train Loss 1.0864 Acc 73.37% | Test Loss 0.8039 Acc 73.67% | Best 73.67%\n",
            "Epoch 017 | LR 0.000258 | Train Loss 1.0579 Acc 74.78% | Test Loss 0.8007 Acc 73.09% | Best 73.67%\n",
            "Epoch 018 | LR 0.000252 | Train Loss 1.0470 Acc 75.33% | Test Loss 0.7331 Acc 76.14% | Best 76.14%\n",
            "Epoch 019 | LR 0.000245 | Train Loss 1.0247 Acc 76.35% | Test Loss 0.7382 Acc 75.97% | Best 76.14%\n",
            "Epoch 020 | LR 0.000239 | Train Loss 1.0127 Acc 76.79% | Test Loss 0.7106 Acc 76.86% | Best 76.86%\n",
            "Epoch 021 | LR 0.000232 | Train Loss 0.9899 Acc 77.93% | Test Loss 0.7068 Acc 76.93% | Best 76.93%\n",
            "Epoch 022 | LR 0.000225 | Train Loss 0.9769 Acc 78.44% | Test Loss 0.7124 Acc 76.92% | Best 76.93%\n",
            "Epoch 023 | LR 0.000218 | Train Loss 0.9602 Acc 79.07% | Test Loss 0.7001 Acc 77.04% | Best 77.04%\n",
            "Epoch 024 | LR 0.000210 | Train Loss 0.9426 Acc 80.01% | Test Loss 0.6705 Acc 78.02% | Best 78.02%\n",
            "Epoch 025 | LR 0.000203 | Train Loss 0.9290 Acc 80.66% | Test Loss 0.6712 Acc 78.23% | Best 78.23%\n",
            "Epoch 026 | LR 0.000195 | Train Loss 0.9062 Acc 81.61% | Test Loss 0.6502 Acc 79.11% | Best 79.11%\n",
            "Epoch 027 | LR 0.000187 | Train Loss 0.8985 Acc 82.15% | Test Loss 0.6399 Acc 79.68% | Best 79.68%\n",
            "Epoch 028 | LR 0.000179 | Train Loss 0.8869 Acc 82.53% | Test Loss 0.6289 Acc 79.96% | Best 79.96%\n",
            "Epoch 029 | LR 0.000171 | Train Loss 0.8678 Acc 83.44% | Test Loss 0.6295 Acc 79.80% | Best 79.96%\n",
            "Epoch 030 | LR 0.000162 | Train Loss 0.8499 Acc 84.15% | Test Loss 0.6064 Acc 80.58% | Best 80.58%\n",
            "Epoch 031 | LR 0.000154 | Train Loss 0.8403 Acc 84.66% | Test Loss 0.6055 Acc 80.71% | Best 80.71%\n",
            "Epoch 032 | LR 0.000146 | Train Loss 0.8206 Acc 85.76% | Test Loss 0.5992 Acc 81.14% | Best 81.14%\n",
            "Epoch 033 | LR 0.000138 | Train Loss 0.8090 Acc 86.24% | Test Loss 0.6025 Acc 81.29% | Best 81.29%\n",
            "Epoch 034 | LR 0.000129 | Train Loss 0.7927 Acc 86.83% | Test Loss 0.5984 Acc 81.58% | Best 81.58%\n",
            "Epoch 035 | LR 0.000121 | Train Loss 0.7824 Acc 87.47% | Test Loss 0.5819 Acc 81.83% | Best 81.83%\n",
            "Epoch 036 | LR 0.000113 | Train Loss 0.7706 Acc 88.04% | Test Loss 0.5936 Acc 81.74% | Best 81.83%\n",
            "Epoch 037 | LR 0.000105 | Train Loss 0.7547 Acc 88.57% | Test Loss 0.5944 Acc 81.74% | Best 81.83%\n",
            "Epoch 038 | LR 0.000097 | Train Loss 0.7417 Acc 89.19% | Test Loss 0.5905 Acc 81.97% | Best 81.97%\n",
            "Epoch 039 | LR 0.000090 | Train Loss 0.7295 Acc 89.85% | Test Loss 0.5963 Acc 81.87% | Best 81.97%\n",
            "Epoch 040 | LR 0.000082 | Train Loss 0.7158 Acc 90.64% | Test Loss 0.5834 Acc 82.26% | Best 82.26%\n",
            "Epoch 041 | LR 0.000075 | Train Loss 0.7073 Acc 90.89% | Test Loss 0.5726 Acc 82.60% | Best 82.60%\n",
            "Epoch 042 | LR 0.000068 | Train Loss 0.6991 Acc 91.16% | Test Loss 0.5791 Acc 82.45% | Best 82.60%\n",
            "Epoch 043 | LR 0.000061 | Train Loss 0.6851 Acc 91.93% | Test Loss 0.5892 Acc 82.56% | Best 82.60%\n",
            "Epoch 044 | LR 0.000055 | Train Loss 0.6746 Acc 92.28% | Test Loss 0.5919 Acc 82.39% | Best 82.60%\n",
            "Epoch 045 | LR 0.000048 | Train Loss 0.6670 Acc 92.74% | Test Loss 0.5977 Acc 82.32% | Best 82.60%\n",
            "Epoch 046 | LR 0.000042 | Train Loss 0.6592 Acc 93.03% | Test Loss 0.5863 Acc 82.72% | Best 82.72%\n",
            "Epoch 047 | LR 0.000037 | Train Loss 0.6502 Acc 93.43% | Test Loss 0.5890 Acc 82.61% | Best 82.72%\n",
            "Epoch 048 | LR 0.000032 | Train Loss 0.6413 Acc 93.93% | Test Loss 0.5941 Acc 82.76% | Best 82.76%\n",
            "Epoch 049 | LR 0.000027 | Train Loss 0.6365 Acc 94.07% | Test Loss 0.5951 Acc 82.72% | Best 82.76%\n",
            "Epoch 050 | LR 0.000022 | Train Loss 0.6299 Acc 94.43% | Test Loss 0.6096 Acc 82.22% | Best 82.76%\n",
            "Epoch 051 | LR 0.000018 | Train Loss 0.6271 Acc 94.54% | Test Loss 0.6004 Acc 82.97% | Best 82.97%\n",
            "Epoch 052 | LR 0.000014 | Train Loss 0.6204 Acc 94.90% | Test Loss 0.6020 Acc 82.99% | Best 82.99%\n",
            "Epoch 053 | LR 0.000011 | Train Loss 0.6156 Acc 95.13% | Test Loss 0.5988 Acc 82.85% | Best 82.99%\n",
            "Epoch 054 | LR 0.000008 | Train Loss 0.6149 Acc 95.17% | Test Loss 0.5995 Acc 82.86% | Best 82.99%\n",
            "Epoch 055 | LR 0.000006 | Train Loss 0.6100 Acc 95.36% | Test Loss 0.6023 Acc 82.99% | Best 82.99%\n",
            "Epoch 056 | LR 0.000004 | Train Loss 0.6082 Acc 95.46% | Test Loss 0.5997 Acc 82.84% | Best 82.99%\n",
            "Epoch 057 | LR 0.000002 | Train Loss 0.6079 Acc 95.49% | Test Loss 0.6017 Acc 82.86% | Best 82.99%\n",
            "Epoch 058 | LR 0.000001 | Train Loss 0.6057 Acc 95.65% | Test Loss 0.6016 Acc 82.75% | Best 82.99%\n",
            "Epoch 059 | LR 0.000000 | Train Loss 0.6064 Acc 95.57% | Test Loss 0.6009 Acc 82.86% | Best 82.99%\n",
            "Epoch 060 | LR 0.000000 | Train Loss 0.6064 Acc 95.53% | Test Loss 0.6008 Acc 82.84% | Best 82.99%\n",
            "Best Test Acc: 0.8299\n"
          ]
        }
      ]
    }
  ]
}